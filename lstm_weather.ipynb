{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants used in pre-processing\n",
    "\n",
    "TEMP_COEF =50\n",
    "\n",
    "PRESS_SHIFT = 1000\n",
    "PRESS_COEF = 100\n",
    "PRESS_DEFAULT = 1000\n",
    "\n",
    "TIME_ZERO = pd.Timestamp('1970-01-01 00:00:00')\n",
    "TIME_DELTA = '1h'\n",
    "\n",
    "SEQ_LENGTH = 48\n",
    "PERIOD_TO_PREDICT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # functions for cleaning the data\n",
    "\n",
    "def preprocess_data(data, val_pct=0.2):\n",
    "    \n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    \n",
    "    pct = data.index[-(int(val_pct * len(data)))]\n",
    "    \n",
    "    print(\"pct:\", pct, \"data.index[0]:\", data.index[0], \"data.index[-1]:\", data.index[-1], \"len(data):\", len(data))\n",
    "    \n",
    "    prev_days_x = deque(maxlen=SEQ_LENGTH)\n",
    "    prev_days_y = deque(maxlen=SEQ_LENGTH)\n",
    "    \n",
    "    for index, row in zip(data.index, data.values):\n",
    "        if index > data.index[-2*PERIOD_TO_PREDICT]:\n",
    "            break\n",
    "        prev_days_x.append([])\n",
    "        prev_days_y.append([])\n",
    "        for n in range(len(row)):\n",
    "            if (n < len(row) / 2):\n",
    "                if type(row[n]) is not tuple:\n",
    "                    prev_days_x[len(prev_days_x) - 1].append(row[n])\n",
    "                else:\n",
    "                    prev_days_x[len(prev_days_x) - 1].extend(row[n])\n",
    "            else:\n",
    "                if type(row[n]) is not tuple:\n",
    "                    prev_days_y[len(prev_days_y) - 1].append(row[n])\n",
    "#                 else:\n",
    "#                     prev_days_y[len(prev_days_y) - 1].extend(row[n])\n",
    "                \n",
    "        if len(prev_days_x) == SEQ_LENGTH:\n",
    "#             if (rand.rand() < val_pct) TODO! RANDOM SPLIT\n",
    "            if index < pct:\n",
    "                train_x.append(np.array(prev_days_x))\n",
    "                train_y.append(np.array(prev_days_y))\n",
    "            else:\n",
    "                val_x.append(np.array(prev_days_x))\n",
    "                val_y.append(np.array(prev_days_y))\n",
    "        \n",
    "    return (np.array(train_x), np.array(train_y)), (np.array(val_x), np.array(val_y))\n",
    "\n",
    "\n",
    "def get_labels(data):\n",
    "    \"\"\" returns the list of distinct labels in given data column \"\"\"\n",
    "    labels = list(set(data))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "def data_to_dicts(labels):\n",
    "    \"\"\" returns pair of data to one-hot and one-hot to data dictionaries \"\"\"\n",
    "    data_to_oh = {x:tuple(1 if y == labels.index(x) else 0 \n",
    "                    for y in range(len(labels))) \n",
    "                    for x in labels}\n",
    "    \n",
    "    oh_to_data = {y:x for x, y in data_to_oh.items()}\n",
    "    \n",
    "    return data_to_oh, oh_to_data\n",
    "\n",
    "\n",
    "def normalize_temp(temp):\n",
    "    return [float(t) / TEMP_COEF for t in temp]\n",
    "\n",
    "\n",
    "def denormalize_temp(temp):\n",
    "    return [t * TEMP_COEF for t in temp]\n",
    "\n",
    "\n",
    "def normalize_press(press):\n",
    "    press = [float(p) for p in press]\n",
    "    for i in range(len(press)):\n",
    "        if press[i] == 0:\n",
    "            press[i] = press[i-1] if i != 0 else PRESS_DEFAULT\n",
    "\n",
    "    return [(p - PRESS_SHIFT) / PRESS_COEF for p in press]\n",
    "\n",
    "\n",
    "def denormalize_press(press):\n",
    "    return [p * PRESS_COEF + PRESS_SHIFT for p in press]\n",
    "\n",
    "\n",
    "def normalize_time(times):\n",
    "    \"\"\" converts date-time data column to a UNIX-style int (number of TIME_DELTA steps since TIME_ZERO) \"\"\"\n",
    "    times = [pd.Timestamp(time[:-6]) for time in times]\n",
    "    times = [((time - TIME_ZERO) // pd.Timedelta(TIME_DELTA)) for time in times]\n",
    "    return times\n",
    "\n",
    "\n",
    "# def denormalize_time(time):\n",
    "# TODO\n",
    "\n",
    "\n",
    "def one_hot_encode(data, data_to_oh):\n",
    "    return [data_to_oh[d] for d in data]\n",
    "\n",
    "\n",
    "def one_hot_decode(oh, oh_to_data):\n",
    "    return [oh_to_data[o] for o in oh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summary' 'precip' 'temp' 'humidity' 'pressure']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"weatherHistory.csv\", names = ['time', 'summary', 'precip', 'temp', 'app_temp', 'humidity', 'wind_speed', 'wind_bearing', 'visibility', 'loud_cover', 'pressure', 'daily_summary'], low_memory=False)\n",
    "\n",
    "df = df.drop([0])\n",
    "df = df.drop(['app_temp', 'wind_speed', 'wind_bearing', 'visibility', 'loud_cover', 'daily_summary'], axis=1) # TODO add wind_speed and other usefull data\n",
    "\n",
    "df.set_index('time', inplace=True)\n",
    "df.index = normalize_time(df.index)\n",
    "\n",
    "df.head()\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_labels = get_labels(df['summary'])\n",
    "# print(\"len(summary_labels):\", len(summary_labels))\n",
    "\n",
    "# our training data contains nans when there is no precipitation\n",
    "df['precip'] = df['precip'].fillna(\"clear\")\n",
    "precip_labels = get_labels(df['precip'])\n",
    "# print(\"len(precip_labels):\", len(precip_labels))\n",
    "\n",
    "# daily_summary_labels = get_labels(df['daily_summary'])\n",
    "# print(\"len(daily_summary_labels):\", len(daily_summary_labels))\n",
    "\n",
    "\n",
    "summary_to_oh, oh_to_summary = data_to_dicts(summary_labels)\n",
    "precip_to_oh, oh_to_precip = data_to_dicts(precip_labels)\n",
    "\n",
    "# print(summary_to_oh, oh_to_summary, sep='\\n\\n')\n",
    "# print(precip_to_oh, oh_to_precip, sep='\\n\\n')\n",
    "\n",
    "df['summary'] = one_hot_encode(df['summary'], summary_to_oh)\n",
    "# df['summary'].head()\n",
    "df['precip'] = one_hot_encode(df['precip'], precip_to_oh)\n",
    "# df['precip'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temp'] = normalize_temp(df['temp'])\n",
    "df['pressure'] = normalize_press(df['pressure'])\n",
    "df['humidity'] = df['humidity'].apply(pd.to_numeric)\n",
    "\n",
    "# print(denormalize_temp(df['temp'])[:5])\n",
    "# print(denormalize_press(df['pressure'])[:5])\n",
    "# print(min(df['temp']), max(df['temp']), '\\n', min(df['pressure']), max(df['pressure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data by index\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we shift values so that each row has a corresponding future row\n",
    "for col in df.columns:\n",
    "    df[\"future_{}\".format(col)] = df[\"{}\".format(col)].shift(-PERIOD_TO_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct: 383071 data.index[0]: 315576 data.index[-1]: 412007 len(data): 96453\n",
      "length of train x: 67471\n",
      "length of train y: 67471\n",
      "length of val x: 28934\n",
      "length of val y: 28934\n",
      "ratio: 0.3001296613246201\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "(train_x, train_y), (val_x, val_y) = preprocess_data(df, 0.3)\n",
    "\n",
    "print(\"length of train x:\", len(train_x))\n",
    "print(\"length of train y:\", len(train_y))\n",
    "print(\"length of val x:\", len(val_x))\n",
    "print(\"length of val y:\", len(val_y))\n",
    "print(\"ratio:\", len(val_x) / (len(train_x) + len(val_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_y = np.zeros(shape=(train_y.shape[0], 144))\n",
    "for i in range(len(train_y)):\n",
    "    new_train_y[i] = train_y[i].ravel()\n",
    "new_val_y = np.zeros(shape=(val_y.shape[0], 144))\n",
    "for i in range(len(val_y)):\n",
    "    new_val_y[i] = val_y[i].ravel()\n",
    "# train_y = train_y.ravel()\n",
    "\n",
    "# print(new_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM\n",
    "# from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants used in the model\n",
    "\n",
    "LSTM_LAYERS = 1\n",
    "LSTM_UNITS = 128\n",
    "\n",
    "FC_LAYERS = 1\n",
    "FC_UNITS = 128\n",
    "\n",
    "INPUT_DIM = (len(summary_labels) + len(precip_labels) + 3) * SEQ_LENGTH\n",
    "# OUTPUT_DIM = len(summary_labels) + len(precip_labels) + 3\n",
    "OUTPUT_DIM = 3 * SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (67471, 48, 33) val_x: (28934, 48, 33)\n",
      "train_y: (67471, 48, 3) val_y: (28934, 48, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\ntrain_x_x = [] # Should be full list\\nfor i in range(SEQ_LENGTH):\\n    prediction = model.predict(train_x_x)\\n    del train_x_x[0]\\n    train_x_x.append(prediction)\\n\\ntrain on train_x_x \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "import time\n",
    "\n",
    "# NAME = \"weater_forecaster_{}\".format(int(time.time()))\n",
    "# tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "print(\"train_x:\", train_x.shape, \"val_x:\", val_x.shape)\n",
    "print(\"train_y:\", train_y.shape, \"val_y:\", val_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# for i in range(LSTM_LAYERS):\n",
    "#     if i == 0:\n",
    "#         if i != LSTM_LAYERS - 1:\n",
    "#             model.add(LSTM(LSTM_UNITS, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "#         else:\n",
    "#             model.add(LSTM(LSTM_UNITS, input_shape=(train_x.shape[1:]), return_sequences=False))\n",
    "#     else:\n",
    "#         if i != LSTM_LAYERS - 1:\n",
    "#             model.add(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "#         else:\n",
    "#             model.add(LSTM(LSTM_UNITS, return_sequences=False))\n",
    "    \n",
    "# for i in range(FC_LAYERS):\n",
    "#     model.add(Dense(FC_UNITS, activation='tanh'))\n",
    "\n",
    "# model.add(Dense(OUTPUT_DIM, activation='tanh'))\n",
    "# model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "# model.fit(train_x, new_train_y, epochs = 20, batch_size=32, validation_data=(val_x, new_val_y), callbacks=[tensorboard])\n",
    "\n",
    "'''\n",
    "TODO\n",
    "train_x_x = [] # Should be full list\n",
    "for i in range(SEQ_LENGTH):\n",
    "    prediction = model.predict(train_x_x)\n",
    "    del train_x_x[0]\n",
    "    train_x_x.append(prediction)\n",
    "\n",
    "train on train_x_x \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
