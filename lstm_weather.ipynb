{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants used in pre-processing\n",
    "\n",
    "TEMP_MEAN = 0\n",
    "TEMP_STD = 0\n",
    "\n",
    "PRESS_MEAN = 0\n",
    "PRESS_STD = 0\n",
    "PRESS_DEFAULT = 1000\n",
    "\n",
    "HUMIDITY_MEAN = 0\n",
    "HUMIDITY_STD = 0\n",
    "\n",
    "TIME_ZERO = pd.Timestamp('1970-01-01 00:00:00')\n",
    "TIME_DELTA = '1h'\n",
    "\n",
    "SEQ_LENGTH = 48\n",
    "TO_PREDICT = 24\n",
    "PERIOD_TO_PREDICT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    " # functions for cleaning the data\n",
    "\n",
    "def preprocess_data(data, val_pct=0.2):\n",
    "    \n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    \n",
    "    pct = data.index[-(int(val_pct * len(data)))]\n",
    "    \n",
    "    print(\"pct:\", pct, \"data.index[0]:\", data.index[0], \"data.index[-1]:\", data.index[-1], \"len(data):\", len(data))\n",
    "    \n",
    "    prev_days_x = deque(maxlen=SEQ_LENGTH)\n",
    "    prev_days_y = deque(maxlen=SEQ_LENGTH)\n",
    "    \n",
    "    for index, row in zip(data.index, data.values):\n",
    "        if index > data.index[-2*PERIOD_TO_PREDICT]:\n",
    "            break\n",
    "        prev_days_x.append([])\n",
    "        prev_days_y.append([])\n",
    "        for n in range(len(row)):\n",
    "            if (n < len(row) / 2):\n",
    "                if type(row[n]) is not tuple:\n",
    "                    prev_days_x[len(prev_days_x) - 1].append(row[n])\n",
    "#                 else:\n",
    "#                     prev_days_x[len(prev_days_x) - 1].extend(row[n])\n",
    "            else:\n",
    "                if type(row[n]) is not tuple:\n",
    "                    prev_days_y[len(prev_days_y) - 1].append(row[n])\n",
    "#                 else:\n",
    "#                     prev_days_y[len(prev_days_y) - 1].extend(row[n])\n",
    "                \n",
    "        if len(prev_days_x) == SEQ_LENGTH:\n",
    "#             if (rand.rand() < val_pct) TODO! RANDOM SPLIT\n",
    "            if index < pct:\n",
    "                train_x.append(np.array(prev_days_x))\n",
    "                train_y.append(np.array(prev_days_y[-1]))\n",
    "            else:\n",
    "                val_x.append(np.array(prev_days_x))\n",
    "                val_y.append(np.array(prev_days_y[-1]))\n",
    "    \n",
    "    # shuffling the data\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(train_x)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(train_y)\n",
    "        \n",
    "    return (np.array(train_x), np.array(train_y)), (np.array(val_x), np.array(val_y))\n",
    "\n",
    "\n",
    "def get_labels(data):\n",
    "    \"\"\" returns the list of distinct labels in given data column \"\"\"\n",
    "    labels = list(set(data))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "def data_to_dicts(labels):\n",
    "    \"\"\" returns pair of data to one-hot and one-hot to data dictionaries \"\"\"\n",
    "    data_to_oh = {x:tuple(1 if y == labels.index(x) else 0 \n",
    "                    for y in range(len(labels))) \n",
    "                    for x in labels}\n",
    "    \n",
    "    oh_to_data = {y:x for x, y in data_to_oh.items()}\n",
    "    \n",
    "    return data_to_oh, oh_to_data\n",
    "\n",
    "\n",
    "# ======= temp =======\n",
    "\n",
    "def normalize_temp(temp):\n",
    "    global TEMP_MEAN, TEMP_STD\n",
    "    TEMP_MEAN = temp.mean()\n",
    "    TEMP_STD = temp.std()\n",
    "    return [(t - TEMP_MEAN) / TEMP_STD for t in temp]\n",
    "\n",
    "\n",
    "def denormalize_temp(temp):\n",
    "    return [t * TEMP_STD + TEMP_MEAN for t in temp]\n",
    "\n",
    "\n",
    "def denormalize_temp_single(temp):\n",
    "    return temp * TEMP_STD + TEMP_MEAN\n",
    "\n",
    "\n",
    "# ======= press =======\n",
    "\n",
    "def normalize_press(press):\n",
    "    global PRESS_MEAN, PRESS_STD\n",
    "    PRESS_MEAN = press.mean()\n",
    "    PRESS_STD = press.std()\n",
    "#     for i in range(len(press)):\n",
    "#         if press[i] == 0:\n",
    "#             press[i] = press[i-1] if i != 0 else PRESS_DEFAULT\n",
    "\n",
    "    return [(PRESS_DEFAULT - PRESS_MEAN) / PRESS_STD if p == 0 else (p - PRESS_MEAN) / PRESS_STD for p in press]\n",
    "\n",
    "\n",
    "def denormalize_press(press):\n",
    "    return [p * PRESS_STD + PRESS_MEAN for p in press]\n",
    "\n",
    "\n",
    "def denormalize_press_single(press):\n",
    "    return press * PRESS_STD + PRESS_MEAN\n",
    "\n",
    "\n",
    "# ======= hum =======\n",
    "\n",
    "def normalize_humidity(hum):\n",
    "    global HUMIDITY_MEAN, HUMIDITY_STD\n",
    "    HUMIDITY_MEAN = hum.mean()\n",
    "    HUMIDITY_STD = hum.std()\n",
    "    return [(h - HUMIDITY_MEAN) / HUMIDITY_STD for h in hum]\n",
    "\n",
    "\n",
    "def denormalize_humidity(hum):\n",
    "    return [h * HUMIDITY_STD + HUMIDITY_MEAN for h in hum]\n",
    "\n",
    "\n",
    "def denormalize_humidity_single(hum):\n",
    "    return hum * HUMIDITY_STD + HUMIDITY_MEAN\n",
    "\n",
    "# ===================\n",
    "\n",
    "def normalize_time(times):\n",
    "    \"\"\" converts date-time data column to a UNIX-style int (number of TIME_DELTA steps since TIME_ZERO) \"\"\"\n",
    "    times = [pd.Timestamp(time[:-6]) for time in times]\n",
    "    times = [((time - TIME_ZERO) // pd.Timedelta(TIME_DELTA)) for time in times]\n",
    "    return times\n",
    "\n",
    "\n",
    "# def denormalize_time(time):\n",
    "# TODO\n",
    "\n",
    "\n",
    "def one_hot_encode(data, data_to_oh):\n",
    "    return [data_to_oh[d] for d in data]\n",
    "\n",
    "\n",
    "def one_hot_decode(oh, oh_to_data):\n",
    "    return [oh_to_data[o] for o in oh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['temp' 'humidity' 'pressure']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"weatherHistory.csv\", names = ['time', 'summary', 'precip', 'temp', 'app_temp', 'humidity', 'wind_speed', 'wind_bearing', 'visibility', 'loud_cover', 'pressure', 'daily_summary'], low_memory=False)\n",
    "\n",
    "df = df.drop([0])\n",
    "df = df.drop(['app_temp', 'wind_speed', 'wind_bearing', 'visibility', 'loud_cover', 'daily_summary'], axis=1) # TODO add wind_speed and other usefull data\n",
    "\n",
    "df = df.drop(['summary', 'precip'], axis=1)\n",
    "\n",
    "df.set_index('time', inplace=True)\n",
    "df.index = normalize_time(df.index)\n",
    "\n",
    "df = df.astype('float32')\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_labels = get_labels(df['summary'])\n",
    "# print(\"len(summary_labels):\", len(summary_labels))\n",
    "\n",
    "# our training data contains nans when there is no precipitation\n",
    "# df['precip'] = df['precip'].fillna(\"clear\")\n",
    "# precip_labels = get_labels(df['precip'])\n",
    "# print(\"len(precip_labels):\", len(precip_labels))\n",
    "\n",
    "# daily_summary_labels = get_labels(df['daily_summary'])\n",
    "# print(\"len(daily_summary_labels):\", len(daily_summary_labels))\n",
    "\n",
    "\n",
    "# summary_to_oh, oh_to_summary = data_to_dicts(summary_labels)\n",
    "# precip_to_oh, oh_to_precip = data_to_dicts(precip_labels)\n",
    "\n",
    "# print(summary_to_oh, oh_to_summary, sep='\\n\\n')\n",
    "# print(precip_to_oh, oh_to_precip, sep='\\n\\n')\n",
    "\n",
    "# df['summary'] = one_hot_encode(df['summary'], summary_to_oh)\n",
    "# df['summary'].head()\n",
    "# df['precip'] = one_hot_encode(df['precip'], precip_to_oh)\n",
    "# df['precip'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: mean=-9.429408123883428e-18, std=1.0\n",
      "pressure: mean=-4.714704061941714e-18, std=1.0\n",
      "humidity: mean=-8.250732108398e-18, std=1.0\n"
     ]
    }
   ],
   "source": [
    "df['temp'] = normalize_temp(df['temp'])\n",
    "df['pressure'] = normalize_press(df['pressure'])\n",
    "df['humidity'] = normalize_humidity(df['humidity'])\n",
    "# df['humidity'] = df['humidity'].apply(pd.to_numeric)\n",
    "\n",
    "print(\"temperature: mean={}, std={}\".format(TEMP_MEAN, TEMP_STD))\n",
    "print(\"pressure: mean={}, std={}\".format(PRESS_MEAN, PRESS_STD))\n",
    "print(\"humidity: mean={}, std={}\".format(HUMIDITY_MEAN, HUMIDITY_STD))\n",
    "\n",
    "# print(denormalize_temp(df['temp'])[:5])\n",
    "# print(denormalize_press(df['pressure'])[:5])\n",
    "# print(min(df['temp']), max(df['temp']), '\\n', min(df['pressure']), max(df['pressure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data by index\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we shift values so that each row has a corresponding future row\n",
    "for col in df.columns:\n",
    "    df[\"future_{}\".format(col)] = df[\"{}\".format(col)].shift(-PERIOD_TO_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct: 382016 data.index[0]: 315576 data.index[-1]: 410500 len(data): 94946\n",
      "length of train x: 66416\n",
      "length of train y: 66416\n",
      "length of val x: 28482\n",
      "length of val y: 28482\n",
      "ratio: 0.30013277413644124\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = df.loc[:410500];\n",
    "(train_x, train_y), (val_x, val_y) = preprocess_data(df, 0.3)\n",
    "\n",
    "print(\"length of train x:\", len(train_x))\n",
    "print(\"length of train y:\", len(train_y))\n",
    "print(\"length of val x:\", len(val_x))\n",
    "print(\"length of val y:\", len(val_y))\n",
    "print(\"ratio:\", len(val_x) / (len(train_x) + len(val_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66416, 48, 3)\n",
      "(66416, 3)\n",
      "(28482, 48, 3)\n",
      "(28482, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants used in the model\n",
    "\n",
    "LSTM_LAYERS = 1\n",
    "LSTM_UNITS = 128\n",
    "\n",
    "FC_LAYERS = 1\n",
    "FC_UNITS = 128\n",
    "\n",
    "# INPUT_DIM = (len(summary_labels) + len(precip_labels) + 3) * SEQ_LENGTH\n",
    "INPUT_DIM = 3 * SEQ_LENGTH\n",
    "# OUTPUT_DIM = 3 * SEQ_LENGTH\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, CuDNNLSTM, Dropout\n",
    "# import time\n",
    "\n",
    "# NAME = \"weater_forecaster_{}\".format(int(time.time()))\n",
    "# tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "for i in range(LSTM_LAYERS):\n",
    "    if i == 0:\n",
    "        if i != LSTM_LAYERS - 1:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "        else:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, input_shape=(train_x.shape[1:]), return_sequences=False))\n",
    "    else:\n",
    "        if i != LSTM_LAYERS - 1:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, return_sequences=True))\n",
    "        else:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "for i in range(FC_LAYERS):\n",
    "    model.add(Dense(FC_UNITS, activation='tanh'))\n",
    "\n",
    "model.add(Dense(OUTPUT_DIM, activation='tanh'))\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, epochs=7, batch_size=BATCH_SIZE, validation_data=(val_x, val_y))\n",
    "# model.fit(train_x, new_train_y, epochs = 20, batch_size=32, validation_data=(val_x, new_val_y), callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 1000\n",
    "test_index = 0\n",
    "\n",
    "def append_to_end(x, val):\n",
    "    for i in range(len(x) - 1):\n",
    "        x[i] = x[i + 1]\n",
    "    x[-1] = val\n",
    "        \n",
    "output1 = model.predict(val_x)\n",
    "output2 = []\n",
    "for i in range(int(test_length / TO_PREDICT)):\n",
    "    x = np.copy(val_x[i * TO_PREDICT])\n",
    "    for j in range(TO_PREDICT):\n",
    "        output2.append(model.predict(x.reshape(-1, 48, 3)).reshape(3,))\n",
    "        append_to_end(x, output2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(val_y[:400])\n",
    "plt.title(\"Real\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(output1[:400])\n",
    "plt.title(\"Soxuw\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(output2[:400])\n",
    "plt.title(\"Sliding\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val_y.shape)\n",
    "real_temp = []\n",
    "output_temp1 = []\n",
    "output_temp2 = []\n",
    "\n",
    "temp_size = 400\n",
    "\n",
    "# for val in val_y[:400]:\n",
    "#     real_temp.append(val[0] * TEMP_COEF)\n",
    "    \n",
    "# for val in output1[:400]:\n",
    "#     output_temp1.append(val[0] * TEMP_COEF)\n",
    "    \n",
    "# for val in output2[:400]:\n",
    "#     output_temp2.append(val[0] * TEMP_COEF)\n",
    "\n",
    "\n",
    "for val in val_y[:temp_size]:\n",
    "    real_temp.append(denormalize_temp_single(val[0]))\n",
    "    \n",
    "for val in output1[:temp_size]:\n",
    "    output_temp1.append(denormalize_temp_single(val[0]))\n",
    "    \n",
    "for val in output2[:temp_size]:\n",
    "    output_temp2.append(denormalize_temp_single(val[0]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(real_temp)\n",
    "plt.title(\"Real\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(output_temp1)\n",
    "plt.title(\"Soxuw\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(output_temp2)\n",
    "plt.title(\"Sliding\")\n",
    "\n",
    "plt.suptitle(\"Temperature\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.plot(real_temp, label=\"Real\")\n",
    "\n",
    "plt.plot(output_temp1, label=\"Soxuw\")\n",
    "\n",
    "plt.plot(output_temp2, label=\"Sliding\")\n",
    "\n",
    "plt.title(\"Temperature\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hm_epochs = 1\n",
    "# rnn_size = 128\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# x = tf.placeholder(tf.float32, [None, SEQ_LENGTH, num_features])\n",
    "# y = tf.placeholder(tf.float32, [None, num_features])\n",
    "\n",
    "# def recurrent_neural_network(x):\n",
    "#     layer = {'weights':tf.Variable(tf.random_normal([rnn_size, num_features])),\n",
    "#              'biases':tf.Variable(tf.random_normal([num_features]))}\n",
    "\n",
    "#     x = tf.transpose(x, [1, 0, 2])\n",
    "#     x = tf.reshape(x, [-1, num_features])\n",
    "#     x = tf.split(x, SEQ_LENGTH, 0)\n",
    "\n",
    "#     lstm_cell = tf.nn.rnn_cell.LSTMCell(rnn_size, state_is_tuple=True)\n",
    "#     outputs, states = tf.nn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "#     output = tf.matmul(outputs[-1], layer['weights']) + layer['biases']\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "# def train_neural_network(x):\n",
    "#     prediction = recurrent_neural_network(x)\n",
    "#     cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y) )\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#         for epoch in range(hm_epochs):\n",
    "#             epoch_loss = 0\n",
    "# #             for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "#             for i in range(int(len(train_x)/batch_size)):\n",
    "# #                 epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "#                 epoch_x = train_x[i * batch_size:(i + 1) * batch_size]\n",
    "#                 epoch_y = train_y[i * batch_size:(i + 1) * batch_size]\n",
    "# #                 epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n",
    "\n",
    "#                 _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "#                 epoch_loss += c\n",
    "\n",
    "#             print('Epoch', epoch + 1, 'completed out of', hm_epochs,'loss:', epoch_loss)\n",
    "\n",
    "# #         correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "# #         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "#         accuracy, _ = tf.metrics.mean_squared_error(labels=y, predictions=prediction)\n",
    "# #         print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_chunks, chunk_size)), y:mnist.test.labels}))\n",
    "# #         print('Accuracy:',accuracy.eval({x:val_x, y:val_y}))\n",
    "#         print('Accuracy:', sess.run(accuracy, feed_dict={x: val_x, y: val_y}))\n",
    "\n",
    "# train_neural_network(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
