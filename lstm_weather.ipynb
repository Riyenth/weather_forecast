{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants used in pre-processing\n",
    "\n",
    "TEMP_COEF = 100\n",
    "\n",
    "PRESS_SHIFT = 1000\n",
    "PRESS_COEF = 100\n",
    "PRESS_DEFAULT = 1000\n",
    "\n",
    "TIME_ZERO = pd.Timestamp('1970-01-01 00:00:00')\n",
    "TIME_DELTA = '1h'\n",
    "\n",
    "SEQ_LENGTH = 48\n",
    "PERIOD_TO_PREDICT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # functions for cleaning the data\n",
    "\n",
    "def preprocess_data(data, val_pct=0.2):\n",
    "    \n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    \n",
    "    pct = data.index[-(int(val_pct * len(data)))]\n",
    "    \n",
    "    print(\"pct:\", pct, \"data.index[0]:\", data.index[0], \"data.index[-1]:\", data.index[-1], \"len(data):\", len(data))\n",
    "    \n",
    "    prev_days_x = deque(maxlen=SEQ_LENGTH)\n",
    "    prev_days_y = deque(maxlen=SEQ_LENGTH)\n",
    "    \n",
    "    for index, row in zip(data.index, data.values):\n",
    "        if index > data.index[-2*PERIOD_TO_PREDICT]:\n",
    "            break\n",
    "        prev_days_x.append([])\n",
    "        prev_days_y.append([])\n",
    "        for n in range(len(row)):\n",
    "            if (n < len(row) / 2):\n",
    "                if type(row[n]) is not tuple:\n",
    "                    prev_days_x[len(prev_days_x) - 1].append(row[n])\n",
    "                else:\n",
    "                    prev_days_x[len(prev_days_x) - 1].extend(row[n])\n",
    "            else:\n",
    "                if type(row[n]) is not tuple:\n",
    "                    prev_days_y[len(prev_days_y) - 1].append(row[n])\n",
    "#                 else:\n",
    "#                     prev_days_y[len(prev_days_y) - 1].extend(row[n])\n",
    "                \n",
    "        if len(prev_days_x) == SEQ_LENGTH:\n",
    "#             if (rand.rand() < val_pct) TODO! RANDOM SPLIT\n",
    "            if index < pct:\n",
    "                train_x.append(np.array(prev_days_x))\n",
    "                train_y.append(np.array(prev_days_y[-1]))\n",
    "            else:\n",
    "                val_x.append(np.array(prev_days_x))\n",
    "                val_y.append(np.array(prev_days_y[-1]))\n",
    "        \n",
    "#         count += 1\n",
    "\n",
    "    return (np.array(train_x), np.array(train_y)), (np.array(val_x), np.array(val_y))\n",
    "\n",
    "\n",
    "def get_labels(data):\n",
    "    \"\"\" returns the list of distinct labels in given data column \"\"\"\n",
    "    labels = list(set(data))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "def data_to_dicts(labels):\n",
    "    \"\"\" returns pair of data to one-hot and one-hot to data dictionaries \"\"\"\n",
    "    data_to_oh = {x:tuple(1 if y == labels.index(x) else 0 \n",
    "                    for y in range(len(labels))) \n",
    "                    for x in labels}\n",
    "    \n",
    "    oh_to_data = {y:x for x, y in data_to_oh.items()}\n",
    "    \n",
    "    return data_to_oh, oh_to_data\n",
    "\n",
    "\n",
    "def normalize_temp(temp):\n",
    "    return [float(t) / TEMP_COEF for t in temp]\n",
    "\n",
    "\n",
    "def denormalize_temp(temp):\n",
    "    return [t * TEMP_COEF for t in temp]\n",
    "\n",
    "\n",
    "def normalize_press(press):\n",
    "    press = [float(p) for p in press]\n",
    "    for i in range(len(press)):\n",
    "        if press[i] == 0:\n",
    "            press[i] = press[i-1] if i != 0 else PRESS_DEFAULT\n",
    "\n",
    "    return [(p - PRESS_SHIFT) / PRESS_COEF for p in press]\n",
    "\n",
    "\n",
    "def denormalize_press(press):\n",
    "    return [p * PRESS_COEF + PRESS_SHIFT for p in press]\n",
    "\n",
    "\n",
    "def normalize_time(times):\n",
    "    \"\"\" converts date-time data column to a UNIX-style int (number of TIME_DELTA steps since TIME_ZERO) \"\"\"\n",
    "    times = [pd.Timestamp(time[:-6]) for time in times]\n",
    "    times = [((time - TIME_ZERO) // pd.Timedelta(TIME_DELTA)) for time in times]\n",
    "    return times\n",
    "\n",
    "\n",
    "# def denormalize_time(time):\n",
    "# TODO\n",
    "\n",
    "\n",
    "def one_hot_encode(data, data_to_oh):\n",
    "    return [data_to_oh[d] for d in data]\n",
    "\n",
    "\n",
    "def one_hot_decode(oh, oh_to_data):\n",
    "    return [oh_to_data[o] for o in oh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summary' 'precip' 'temp' 'humidity' 'pressure']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"weatherHistory.csv\", names = ['time', 'summary', 'precip', 'temp', 'app_temp', 'humidity', 'wind_speed', 'wind_bearing', 'visibility', 'loud_cover', 'pressure', 'daily_summary'], low_memory=False)\n",
    "\n",
    "df = df.drop([0])\n",
    "df = df.drop(['app_temp', 'wind_speed', 'wind_bearing', 'visibility', 'loud_cover', 'daily_summary'], axis=1) # TODO\n",
    "\n",
    "df.set_index('time', inplace=True)\n",
    "df.index = normalize_time(df.index)\n",
    "\n",
    "df.head()\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_labels = get_labels(df['summary'])\n",
    "# print(\"len(summary_labels):\", len(summary_labels))\n",
    "\n",
    "# our training data contains nans when there is no precipitation\n",
    "df['precip'] = df['precip'].fillna(\"clear\")\n",
    "precip_labels = get_labels(df['precip'])\n",
    "# print(\"len(precip_labels):\", len(precip_labels))\n",
    "\n",
    "# daily_summary_labels = get_labels(df['daily_summary'])\n",
    "# print(\"len(daily_summary_labels):\", len(daily_summary_labels))\n",
    "\n",
    "\n",
    "summary_to_oh, oh_to_summary = data_to_dicts(summary_labels)\n",
    "precip_to_oh, oh_to_precip = data_to_dicts(precip_labels)\n",
    "\n",
    "# print(summary_to_oh, oh_to_summary, sep='\\n\\n')\n",
    "# print(precip_to_oh, oh_to_precip, sep='\\n\\n')\n",
    "\n",
    "df['summary'] = one_hot_encode(df['summary'], summary_to_oh)\n",
    "# df['summary'].head()\n",
    "df['precip'] = one_hot_encode(df['precip'], precip_to_oh)\n",
    "# df['precip'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temp'] = normalize_temp(df['temp'])\n",
    "df['pressure'] = normalize_press(df['pressure'])\n",
    "df['humidity'] = df['humidity'].apply(pd.to_numeric)\n",
    "\n",
    "# print(denormalize_temp(df['temp'])[:5])\n",
    "# print(denormalize_press(df['pressure'])[:5])\n",
    "# print(min(df['temp']), max(df['temp']), '\\n', min(df['pressure']), max(df['pressure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data by index\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we shift values so that each row has a corresponding future row\n",
    "for col in df.columns:\n",
    "    df[\"future_{}\".format(col)] = df[\"{}\".format(col)].shift(-PERIOD_TO_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct: 383071 data.index[0]: 315576 data.index[-1]: 412007 len(data): 96453\n",
      "length of train x: 67471\n",
      "length of train y: 67471\n",
      "length of val x: 28912\n",
      "length of val y: 28912\n",
      "ratio: 0.29996991170642123\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "(train_x, train_y), (val_x, val_y) = preprocess_data(df, 0.3)\n",
    "\n",
    "print(\"length of train x:\", len(train_x))\n",
    "print(\"length of train y:\", len(train_y))\n",
    "print(\"length of val x:\", len(val_x))\n",
    "print(\"length of val y:\", len(val_y))\n",
    "print(\"ratio:\", len(val_x) / (len(train_x) + len(val_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants used in the model\n",
    "\n",
    "LSTM_LAYERS = 2\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "FC_LAYERS = 1\n",
    "FC_UNITS = 256\n",
    "\n",
    "INPUT_DIM = (len(summary_labels) + len(precip_labels) + 3) * SEQ_LENGTH\n",
    "# OUTPUT_DIM = len(summary_labels) + len(precip_labels) + 3\n",
    "OUTPUT_DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (67471, 48, 33) val_x: (28912, 48, 33)\n",
      "train_y: (67471, 3) val_y: (28912, 3)\n",
      "Train on 67471 samples, validate on 28912 samples\n",
      "Epoch 1/20\n",
      "67471/67471 [==============================] - 80s 1ms/step - loss: 0.6599 - acc: 0.5324 - val_loss: 0.5764 - val_acc: 0.5862\n",
      "Epoch 2/20\n",
      "67471/67471 [==============================] - 67s 988us/step - loss: 0.5623 - acc: 0.5513 - val_loss: 0.5311 - val_acc: 0.5935\n",
      "Epoch 3/20\n",
      "67471/67471 [==============================] - 75s 1ms/step - loss: 0.5406 - acc: 0.5522 - val_loss: 0.5253 - val_acc: 0.5935\n",
      "Epoch 4/20\n",
      "67471/67471 [==============================] - 82s 1ms/step - loss: 0.5370 - acc: 0.5522 - val_loss: 0.5240 - val_acc: 0.5935\n",
      "Epoch 5/20\n",
      "58848/67471 [=========================>....] - ETA: 11s - loss: 0.5356 - acc: 0.5520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3bfd1561a6cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUTPUT_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_absolute_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\imran\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\imran\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    212\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\imran\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2977\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2978\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2979\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\imran\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "NAME = \"DUDUW\"\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "print(\"train_x:\", train_x.shape, \"val_x:\", val_x.shape)\n",
    "print(\"train_y:\", train_y.shape, \"val_y:\", val_y.shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "for i in range(LSTM_LAYERS):\n",
    "    if i == 0:\n",
    "        if i != LSTM_LAYERS - 1:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "        else:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, input_shape=(train_x.shape[1:]), return_sequences=False))\n",
    "    else:\n",
    "        if i != LSTM_LAYERS - 1:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, return_sequences=True))\n",
    "        else:\n",
    "            model.add(CuDNNLSTM(LSTM_UNITS, return_sequences=False))\n",
    "    \n",
    "for i in range(FC_LAYERS):\n",
    "    model.add(Dense(FC_UNITS, activation='tanh'))\n",
    "\n",
    "model.add(Dense(OUTPUT_DIM, activation='tanh'))\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, epochs = 20, batch_size=32, validation_data=(val_x, val_y), callbacks=[tensorboard])\n",
    "\n",
    "'''\n",
    "TODO\n",
    "train_x_x = [] # Should be full list\n",
    "for i in range(SEQ_LENGTH):\n",
    "    prediction = model.predict(train_x_x)\n",
    "    del train_x_x[0]\n",
    "    train_x_x.append(prediction)\n",
    "\n",
    "train on train_x_x \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
